{
 "cells": [
  {
   "source": [
    "### Important: Set up directories\n",
    "Set `WORK_DIR` to the path to the repo in the cell below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "WORK_DIR = os.path.join(os.getenv(\"HOME\"), 'text-gnn')\n",
    "os.chdir(WORK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:80% !important; }</style>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "import json\n",
    "import jsonlines\n",
    "import torch\n",
    "\n",
    "sys.path.append('src')\n",
    "from src.shared.utils import tokenize_prune_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helsinki Swahili Corpus\n",
    "Set the `DATASET_DIR_NAME` and `VOCAB_DIR_NAME` variables to the directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results from the `create_dataset.py` script\n",
    "DATASET_DIR_NAME = 'swahili-processed-v1'\n",
    "# Results from running the `download_stemming.py` script\n",
    "VOCAB_DIR_NAME = 'hsc-dictionary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents\n",
    "Explore document stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(WORK_DIR, 'results', DATASET_DIR_NAME)\n",
    "df = pd.read_csv(f'{dataset_dir}/dataset.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "457 total documents with 3 classes\n"
     ]
    }
   ],
   "source": [
    "n_classes = df.document_type.nunique()\n",
    "print(f'{len(df)} total documents with {n_classes} classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "news     221\n",
       "bunge    199\n",
       "books     37\n",
       "Name: document_type, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.document_type.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_words'] = df.document_content.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_words = round(df.n_words.mean(),1)\n",
    "median_words = round(df.n_words.median(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean words per document of 52887.8\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean words per document of {mean_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Median words per document of 38732.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Median words per document of {median_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24.17 million words total\n"
     ]
    }
   ],
   "source": [
    "total_words = df.n_words.sum()\n",
    "print(f'{total_words/int(1e6):.2f} million words total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File size is 160.1 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "Explore vocabulary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_dir = os.path.join(WORK_DIR, 'results', VOCAB_DIR_NAME, 'stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_vocab(path: str, count_threshold: int):\n",
    "    with open(path,'r') as f:\n",
    "        vocab_counts = json.load(f)\n",
    "        return [word for word, count in vocab_counts.items() if count >= count_threshold]"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "434449 unique words in the unstemmed vocabulary\n",
      "212920 unique words in the unstemmed vocabulary which occur at least twice\n"
     ]
    }
   ],
   "source": [
    "unstemmed_vocab_path = os.path.join(stemming_dir, 'vocab_counts.json')\n",
    "unstemmed_vocab = get_words_in_vocab(unstemmed_vocab_path, count_threshold=1)\n",
    "print(f'{len(unstemmed_vocab)} unique words in the unstemmed vocabulary')\n",
    "unstemmed_vocab = get_words_in_vocab(unstemmed_vocab_path, count_threshold=2)\n",
    "print(f'{len(unstemmed_vocab)} unique words in the unstemmed vocabulary which occur at least twice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Still need to generate the cleaned HSC vocab and stemming map\n",
    "stemmed_vocab_path = os.path.join(stemming_dir, 'cleaned_vocab_counts.json')\n",
    "stemmed_vocab = get_words_in_vocab(stemmed_vocab_path, count_threshold=1)\n",
    "print(f'{len(stemmed_vocab)} unique words in the stemmed vocabulary')\n",
    "stemmed_vocab_2 = get_words_in_vocab(stemmed_vocab_path, count_threshold=2)\n",
    "print(f'{len(stemmed_vocab)} unique words in the unstemmed vocabulary which occur at least twice')"
   ]
  },
  {
   "source": [
    "Which we should be able to check is the same as applying this method of finding the number of words in the vocab after applying stemming and cleaning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Requires cell above\n",
    "stemmer_path = os.path.join(stemming_dir, 'stemming_cleaned.json')\n",
    "with open(stemmer_path,'r') as f:\n",
    "    stemming_map = json.load(f)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in stemmed_vocab_2:\n",
    "    stemmed_words.extend(tokenize_prune_stem(word, stemming_map))\n",
    "print(f'{len(set(stemmed_words))} words in vocab after applying stemming and pruning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer_path = os.path.join(stemming_dir, 'stemming_cleaned.json')\n",
    "# with open(stemmer_path,'r') as f:\n",
    "#     stemming_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed_words = [stemming_map[word] for word in stemmed_vocab]\n",
    "# print(f'{len(set(stemmed_words))} words in vocab after applying stemming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenodo Swahili News Corpus\n",
    "Set the `DATASET_DIR_NAME` and `VOCAB_DIR_NAME` variables to the directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results from the `create_dataset.py` script\n",
    "DATASET_DIR_NAME = 'zenodo-processed-data-v2'\n",
    "# Results from running the `download_stemming.py` script\n",
    "VOCAB_DIR_NAME = 'z-news-dictionary-ct2-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents\n",
    "Explore document stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = f'{WORK_DIR}/results/zen_data'\n",
    "dataset_dir = os.path.join(WORK_DIR, 'results', DATASET_DIR_NAME)\n",
    "df = pd.read_csv(f'{dataset_dir}/dataset.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23266 total documents with 6 classes\n"
     ]
    }
   ],
   "source": [
    "n_classes = df.document_type.nunique()\n",
    "print(f'{len(df)} total documents with {n_classes} classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "kitaifa      10242\n",
       "michezo       6003\n",
       "burudani      2229\n",
       "uchumi        2027\n",
       "kimataifa     1906\n",
       "afya           859\n",
       "Name: document_type, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.document_type.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       id/path                                   document_content  \\\n",
       "0       SW4670  bodi ya utalii tanzania (ttb) imesema, itafany...   \n",
       "1      SW30826  pendo fundisha-mbeya rais dk. john magufuri, a...   \n",
       "2      SW29725  mwandishi wetu -singida benki ya nmb imetoa ms...   \n",
       "3      SW20901  timu ya taifa ya tanzania, serengeti boys jana...   \n",
       "4      SW12560  na agatha charles - dar es salaam aliyekuwa ka...   \n",
       "...        ...                                                ...   \n",
       "23261  SW24920  alitoa pongezi hizo alipozindua rasmi hatua ya...   \n",
       "23262   SW4038  na nora damian-dar es salaam  tekla (si jina l...   \n",
       "23263  SW16649  mkuu wa mkoa wa njombe, dk rehema nchimbi waka...   \n",
       "23264  SW23291  mabingwa wa ligi kuu soka tanzania bara, simba...   \n",
       "23265  SW11778  wiki iliyopita, nilianza makala haya yanayolen...   \n",
       "\n",
       "      document_type  document_idx  label_idx  \n",
       "0            uchumi             0          0  \n",
       "1           kitaifa             1          1  \n",
       "2            uchumi             2          0  \n",
       "3           michezo             3          2  \n",
       "4           kitaifa             4          1  \n",
       "...             ...           ...        ...  \n",
       "23261        uchumi         23261          0  \n",
       "23262       kitaifa         23262          1  \n",
       "23263        uchumi         23263          0  \n",
       "23264       michezo         23264          2  \n",
       "23265       kitaifa         23265          1  \n",
       "\n",
       "[23266 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id/path</th>\n      <th>document_content</th>\n      <th>document_type</th>\n      <th>document_idx</th>\n      <th>label_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SW4670</td>\n      <td>bodi ya utalii tanzania (ttb) imesema, itafany...</td>\n      <td>uchumi</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SW30826</td>\n      <td>pendo fundisha-mbeya rais dk. john magufuri, a...</td>\n      <td>kitaifa</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SW29725</td>\n      <td>mwandishi wetu -singida benki ya nmb imetoa ms...</td>\n      <td>uchumi</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SW20901</td>\n      <td>timu ya taifa ya tanzania, serengeti boys jana...</td>\n      <td>michezo</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SW12560</td>\n      <td>na agatha charles - dar es salaam aliyekuwa ka...</td>\n      <td>kitaifa</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23261</th>\n      <td>SW24920</td>\n      <td>alitoa pongezi hizo alipozindua rasmi hatua ya...</td>\n      <td>uchumi</td>\n      <td>23261</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23262</th>\n      <td>SW4038</td>\n      <td>na nora damian-dar es salaam  tekla (si jina l...</td>\n      <td>kitaifa</td>\n      <td>23262</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23263</th>\n      <td>SW16649</td>\n      <td>mkuu wa mkoa wa njombe, dk rehema nchimbi waka...</td>\n      <td>uchumi</td>\n      <td>23263</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23264</th>\n      <td>SW23291</td>\n      <td>mabingwa wa ligi kuu soka tanzania bara, simba...</td>\n      <td>michezo</td>\n      <td>23264</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>23265</th>\n      <td>SW11778</td>\n      <td>wiki iliyopita, nilianza makala haya yanayolen...</td>\n      <td>kitaifa</td>\n      <td>23265</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>23266 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_length'] = df.document_content.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               document_idx  label_idx   doc_length\n",
       "document_type                                      \n",
       "afya           11258.990687        5.0  3041.287544\n",
       "burudani       11765.022432        4.0  1445.668013\n",
       "kimataifa      11535.479538        3.0  1984.798006\n",
       "kitaifa        11714.516696        1.0  2618.339582\n",
       "michezo        11572.384641        2.0  1816.158921\n",
       "uchumi         11499.905279        0.0  2070.512580"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document_idx</th>\n      <th>label_idx</th>\n      <th>doc_length</th>\n    </tr>\n    <tr>\n      <th>document_type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>afya</th>\n      <td>11258.990687</td>\n      <td>5.0</td>\n      <td>3041.287544</td>\n    </tr>\n    <tr>\n      <th>burudani</th>\n      <td>11765.022432</td>\n      <td>4.0</td>\n      <td>1445.668013</td>\n    </tr>\n    <tr>\n      <th>kimataifa</th>\n      <td>11535.479538</td>\n      <td>3.0</td>\n      <td>1984.798006</td>\n    </tr>\n    <tr>\n      <th>kitaifa</th>\n      <td>11714.516696</td>\n      <td>1.0</td>\n      <td>2618.339582</td>\n    </tr>\n    <tr>\n      <th>michezo</th>\n      <td>11572.384641</td>\n      <td>2.0</td>\n      <td>1816.158921</td>\n    </tr>\n    <tr>\n      <th>uchumi</th>\n      <td>11499.905279</td>\n      <td>0.0</td>\n      <td>2070.512580</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df.groupby(['document_type']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               document_idx  label_idx  doc_length\n",
       "document_type                                     \n",
       "afya                10963.0        5.0      2252.0\n",
       "burudani            11593.0        4.0       893.0\n",
       "kimataifa           11401.5        3.0      1542.0\n",
       "kitaifa             11814.0        1.0      2148.5\n",
       "michezo             11566.0        2.0      1577.0\n",
       "uchumi              11555.0        0.0      2075.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document_idx</th>\n      <th>label_idx</th>\n      <th>doc_length</th>\n    </tr>\n    <tr>\n      <th>document_type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>afya</th>\n      <td>10963.0</td>\n      <td>5.0</td>\n      <td>2252.0</td>\n    </tr>\n    <tr>\n      <th>burudani</th>\n      <td>11593.0</td>\n      <td>4.0</td>\n      <td>893.0</td>\n    </tr>\n    <tr>\n      <th>kimataifa</th>\n      <td>11401.5</td>\n      <td>3.0</td>\n      <td>1542.0</td>\n    </tr>\n    <tr>\n      <th>kitaifa</th>\n      <td>11814.0</td>\n      <td>1.0</td>\n      <td>2148.5</td>\n    </tr>\n    <tr>\n      <th>michezo</th>\n      <td>11566.0</td>\n      <td>2.0</td>\n      <td>1577.0</td>\n    </tr>\n    <tr>\n      <th>uchumi</th>\n      <td>11555.0</td>\n      <td>0.0</td>\n      <td>2075.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df.groupby(['document_type']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train samples: 18612\nNumber of val samples: 2327\nNumber of test samples: 2327\n"
     ]
    }
   ],
   "source": [
    "train_labels = torch.load(os.path.join(dataset_dir, 'train-labels.pt'))\n",
    "val_labels = torch.load(os.path.join(dataset_dir, 'val-labels.pt'))\n",
    "test_labels = torch.load(os.path.join(dataset_dir, 'test-labels.pt'))\n",
    "\n",
    "print(f'Number of train samples: {train_labels.shape[0]}')\n",
    "print(f'Number of val samples: {val_labels.shape[0]}')\n",
    "print(f'Number of test samples: {test_labels.shape[0]}')\n",
    "\n",
    "labels_map_path = os.path.join(dataset_dir, 'labels.json')\n",
    "with open(labels_map_path, 'r') as f:\n",
    "    labels_map = json.load(f)\n",
    "reverse_labels_map = {label_idx: label_name for label_name, label_idx in labels_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train doc counts per class: {'uchumi': 1622, 'kitaifa': 8193, 'michezo': 4802, 'kimataifa': 1525, 'burudani': 1783, 'afya': 687}\n"
     ]
    }
   ],
   "source": [
    "categories, counts, = train_labels.unique(return_counts=True)\n",
    "train_counts = {reverse_labels_map[cat]: count for cat, count in zip(categories.tolist(), counts.tolist())}\n",
    "print(f'Train doc counts per class: {train_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'uchumi': 202, 'kitaifa': 1024, 'michezo': 601, 'kimataifa': 191, 'burudani': 223, 'afya': 86}\nVal doc counts per class: {'uchumi': 202, 'kitaifa': 1024, 'michezo': 601, 'kimataifa': 191, 'burudani': 223, 'afya': 86}\n"
     ]
    }
   ],
   "source": [
    "categories, counts, = val_labels.unique(return_counts=True)\n",
    "val_counts = {reverse_labels_map[cat]: count for cat, count in zip(categories.tolist(), counts.tolist())}\n",
    "print(val_counts)\n",
    "print(f'Val doc counts per class: {val_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test doc counts per class: {'uchumi': 203, 'kitaifa': 1025, 'michezo': 600, 'kimataifa': 190, 'burudani': 223, 'afya': 86}\n"
     ]
    }
   ],
   "source": [
    "categories, counts, = test_labels.unique(return_counts=True)\n",
    "test_counts = {reverse_labels_map[cat]: count for cat, count in zip(categories.tolist(), counts.tolist())}\n",
    "print(f'Test doc counts per class: {test_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_words'] = df.document_content.apply(lambda x:len(x.split()))\n",
    "mean_words = round(df.n_words.mean(),1)\n",
    "median_words = round(df.n_words.median(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean words per document of 332.2\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean words per document of {mean_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Median words per document of 275.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Median words per document of {median_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.73 million words total\n"
     ]
    }
   ],
   "source": [
    "total_words = df.n_words.sum()\n",
    "print(f'{total_words/int(1e6):.2f} million words total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File size is 52.3 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "Explore vocabulary stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_dir = os.path.join(WORK_DIR, 'results', VOCAB_DIR_NAME, 'stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "197619 unique words in the unstemmed vocabulary\n95504 unique words in the unstemmed vocabulary which occur at least twice\n"
     ]
    }
   ],
   "source": [
    "unstemmed_vocab_path = os.path.join(stemming_dir, 'vocab_counts.json')\n",
    "unstemmed_vocab = get_words_in_vocab(unstemmed_vocab_path, count_threshold=1)\n",
    "print(f'{len(unstemmed_vocab)} unique words in the unstemmed vocabulary')\n",
    "unstemmed_vocab = get_words_in_vocab(unstemmed_vocab_path, count_threshold=2)\n",
    "print(f'{len(unstemmed_vocab)} unique words in the unstemmed vocabulary which occur at least twice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34467 unique words in the stemmed vocabulary\n34467 unique words in the unstemmed vocabulary which occur at least twice\n"
     ]
    }
   ],
   "source": [
    "# TODO: Still need to generate the cleaned HSC vocab and stemming map\n",
    "stemmed_vocab_path = os.path.join(stemming_dir, 'cleaned_vocab_counts.json')\n",
    "stemmed_vocab = get_words_in_vocab(stemmed_vocab_path, count_threshold=1)\n",
    "print(f'{len(stemmed_vocab)} unique words in the stemmed vocabulary')\n",
    "stemmed_vocab_2 = get_words_in_vocab(stemmed_vocab_path, count_threshold=2)\n",
    "print(f'{len(stemmed_vocab)} unique words in the unstemmed vocabulary which occur at least twice')"
   ]
  },
  {
   "source": [
    "We now consider the remaining words after removing non-alpha's, single character words, and stopwords "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31852 words in vocab after applying stemming and pruning\n"
     ]
    }
   ],
   "source": [
    "stemmer_path = os.path.join(stemming_dir, 'stemming_cleaned.json')\n",
    "with open(stemmer_path,'r') as f:\n",
    "    stemming_map = json.load(f)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in stemmed_vocab_2:\n",
    "    stemmed_words.extend(tokenize_prune_stem(word, stemming_map))\n",
    "print(f'{len(set(stemmed_words))} words in vocab after applying stemming and pruning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('GraphEnv': venv)",
   "language": "python",
   "name": "python36964bitgraphenvvenve4b9ac01a97242c3a48a5a806f9506fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}